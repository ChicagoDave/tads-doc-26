I think Claude Code is very impressive for some things, but actually producing code is absolutely not one of them.

Perhaps I’m approaching things differently from people who have better experiences. For one I’m not approaching my interactions as if I’m talking to a “partner”, much less a romantic partner (in “couple’s therapy”, which sounds more like a Black Mirror plot than a way to get a clear-eyed evaluation of the system’s capabilities). And, as I commented before, I actually try asking questions I already know the answer to. And Claude Code constantly produces just outright incorrect or at best subtilty bad code.

There are some things I’ve noticed that are just stylistic complaints: writing a method that tests for some condition and either returns a default or does some processing before returning a different value, and instead of putting the check-and-return-default at the top, Claude produces a giant conditional block.

I’ve also had the experience where I caution that any answer should be subjected to whatever static analysis, call graph analysis, and so on that Claude is capable of doing (I’m not going to include the entire spell-like invocation rituals for doing this but we can talk about that (and the fact that, like spells, these things do not have deterministic results, which I consider a strict liability in this kind of thing) if you want). And then I ask it a question and it provides an answer I can immediately see is structurally wrong. So I say, there is an obvious problem with this code, what is it? And it thinks for a second and tells me what’s wrong with it. At which point I ask how it managed to provide an obviously incorrect answer when I told it to do all the static analysis it could before suggesting an answer. And it says sorry, but it looked at the class interface and read the comments and thought that was good enough.

Like. If I had hired a junior programmer who constantly did that kind of thing I’d fire them.

I mean there are also very cool things Claude can do. Like you point them to tens of thousand lines of code and then you can get very detailed and as near as I can tell reliable information about the call graph, for example. And that’s cool! Doing that “manually” takes me at least many minutes and sometimes hours for large codebases, and Claude can do it in seconds.

But even the “easy” things have to be constantly checked. If you actually, you know, care about the quality of the code it produces.

And it seems to be even worse at “big” things that involve re-scoping or simultaneously tweaking multiple independent structural elements of the code. Like I asked a question about modifying the TADS3 parser that requires tweaking both very early and very late things that happen in the process. And it very confidently proposed the naive solution (a single tweak that doesn’t work at all, because the TADS3 documentation is somewhat misleading and it relied on its understanding of the documentation instead of what the code actually does). After correcting this, it created a different, more subtle false solution which worked for the specific test case it had devised for it but which failed for the general specification (which, again, it was capable of detecting when prompted). At which point it recognized that it was only coding to solve part of the problem, scrapped its earlier work, and came up with a “solution” that only fixed the other half of the problem. Continued prompting chases Claude in a loop, where it keeps proposing solutions that fix individual parts of the problem without figuring out that it needs to implement several different tweaks in several different places. Eventually circling around to propose a slightly differently-structured version of the original solution. Which, again, it was capable of recognizing when prompted.

To paraphrase Douglas Adams, my experience with AI code generation is that it’s easy to be blinded to the poor quality of their code by the novelty of getting them to produce working code at all.